#These instructions are dated - will not work with current challenge reqiurements as of 1/18/2022

# Challenge Scenario

# As a junior data engineer in Jooli Inc. and recently trained with Google Cloud and a number of data services you have been asked to demonstrate your newly learned skills. 
# The team has asked you to complete the following tasks.
# You are expected to have the skills and knowledge for these tasks so don’t expect step-by-step guides.

# Refer to https://www.cloudskillsboost.google/focuses/11044?parent=catalog for all task requirements and details.

# Task 1: Run a Dataflow job.  

# Steps:
  
  Create a BigQuery dataset called lab
  
  Go back to the Cloud Console, select the new dataset lab and click Create Table.
  
  In the Create Table dialog, select Google Cloud Storage from the dropdown in the Source section.
  
  Copy gs://cloud-training/gsp323/lab.csv to Select file from GCS bucket.
  
  Enter customers to “Table name” in the Destination section.
  
  Enable Edit as text and copy the JSON data from the lab.schema file to the textarea in the Schema section.
  
  Click Create table.
  
  
  Create a Cloud Storage bucket
  
    In Cloud console go to Navigation Menu > Storage
    
    Click Create Bucket
    
    Use GCP Project ID to Name the bucket
    
    Click Create
    
    
    Create a Dataglow job
    
    In the Cloud Console, click on Navigation Menu > Dataflow.

    Click CREATE JOB FROM TEMPLATE.

    In Create job from the template, give an arbitrary job name.


    From the dropdown under Dataflow template, select Text Files on Cloud Storage to BigQuery under 
    “Process Data in Bulk (batch)”. (DO NOT select the item under “Process Data Continuously (stream)”).
    
    Under the Required parameters, enter the following values:

    Field	Value

    JavaScript UDF path in Cloud Storage	gs://cloud-training/gsp323/lab.js

    JSON path	gs://cloud-training/gsp323/lab.schema

    JavaScript UDF name	transform

    BigQuery output table	YOUR_PROJECT:lab.customers

    Cloud Storage input path	gs://cloud-training/gsp323/lab.csv

    Temporary BigQuery directory	gs://YOUR_PROJECT/bigquery_temp

    Temporary location	gs://YOUR_PROJECT/temp

    Replace YOUR_PROJECT with your project ID.
    
    Click Run Job

# Task 2: Run a Dataproc job.

# Steps:
  
   Navigation Menu > Dataproc > Clusters
   
   Click Create Cluster.  Set region to us-central1.  Create  
   
   Open SSH.  Run command
   
   hdfs dfs -cp gs://cloud-training/gsp323/data.txt /data.txt
   
   Close SSH.  Return to Cloud Console.
   
   Click Submit Job.  Select 
   
   Spark for Job type
    
   org.apache.spark.examples.SparkPageRank to "Main class or jar" 
    
   file:///usr/lib/spark/examples/jars/spark-examples.jar to "Jar files"
    
   /data.txt to "Arguments"
    
   Click Create



# Task 3: Run a Dataprep job.

# Steps:
  Create Cloud Storage bucket: Navigation menu > Cloud Storage > Browser.  Refer to guidelines for bucket creation parameters.
  
  Initialize Cloud Dataprep.  Navagation menu > Dataprep > Flows > Create > Blank Flow > Add Datasets > Import Datasets > import gs://cloud-training/gsp323/runs.csv file
  
  After launching the Dataperop Transformer, scroll right to the end and select column10.

  In the Details pane, click FAILURE under Unique Values to show the context menu.
  
  Select Delete rows with selected values to Remove all rows with the state of “FAILURE”.  
  
  Click the downward arrow next to column9, choose Filter rows > On column value > Contains.

  In the Filter rows pane, enter the regex pattern /(^0$|^0\.0$)/ to “Pattern to match”.

  Select Delete matching rows under the Action section, then click the Add button.
  
  Rename the columns to be:

    runid
    userid
    labid
    lab_title
    start
    end
    time
    score
    state

  Confirm the recipe. It should like the screenshot below.

  Click Run Job

# Task 4: AI

  Use Google Cloud Speech API to analyze the audio file
  
  In the Cloud Console, click on Navigation menu > APIs & Services > Credentials.
  
  In the Credentials page, click on + CREATE CREDENTIALS > API key.
  
  Copy the API key to the clipboard, then click RESTRICT KEY.
  
  Open the Cloud Shell, store the API key as an environment variable by running the following command:
  
  export API_KEY=<YOUR-API-KEY>
  
  Replace <YOUR-API-KEY> with the copied key value.

  In the Cloud Shell, create a JSON file called 
  
  gsc-request.json.
  
  Save the following codes to the file:
  
  {
  "config": {
      "encoding":"FLAC",
      "languageCode": "en-US"
  },
  "audio": {
      "uri":"gs://cloud-training/gsp323/task4.flac"
  }
}

  Use the following curl command to submit the request to Google Cloud Speech API and store the response to a file called 
  
  task4-gcs.result
  
  curl -s -X POST -H "Content-Type: application/json" --data-binary @gsc-request.json \
"https://speech.googleapis.com/v1/speech:recognize?key=${API_KEY}" > task4-gcs.result

Upload the resulted file to Cloud Storage by running:

gsutil cp task4-gcs.result gs://<YOUR-PROJECT_ID>-marking/task4-gcs.result

Replace <YOUR-PROJECT_ID> with your project ID.


Use the Cloud Natural Language API to analyze the sentence

In the Cloud Shell, run the following command to use the Cloud Natural Language API to analyze the given sentence.

gcloud ml language analyze-entities --content="Old Norse texts portray Odin as one-eyed and long-bearded, frequently wielding a spear named Gungnir and wearing a cloak and a broad hat." > task4-cnl.result

Upload the resulted file to Cloud Storage by running:

gsutil cp task4-cnl.result gs://<YOUR-PROJECT_ID>-marking/task4-cnl.result
Replace <YOUR-PROJECT_ID> with your project ID.

Use Google Video Intelligence and detect all text on the video
In the Cloud Shell, create a JSON file called 

gvi-request.json

Save the following codes to the file.

{
   "inputUri":"gs://spls/gsp154/video/train.mp4",
   "features": [
       "LABEL_DETECTION"
   ]
}
Go back to the Cloud Console, click on Navigation menu > APIs & Services > Credentials.

Click the service account named “Qwiklabs User Service Account” to view the details.

Click ADD KEY > Create new key.

Choose JSON and click CREATE to download the Private key file to your computer.

Upload the file to the Cloud Shell environment.

Rename the uploaded file to key.json.

Run the following commands to create a token.

gcloud auth activate-service-account --key-file key.json
export TOKEN=$(gcloud auth print-access-token)

Run the following command to use theGoogle Video Intelligence and detect all text on the video.

curl -s -H 'Content-Type: application/json' \
   -H 'Authorization: Bearer '$(gcloud auth print-access-token)'' \
   'https://videointelligence.googleapis.com/v1/videos:annotate' \
   -d @gvi-request.json > task4-gvi.result

Upload the resulted file to Cloud Storage by running:

gsutil cp task4-gvi.result gs://<YOUR-PROJECT_ID>-marking/task4-gvi.result

Replace <YOUR-PROJECT_ID> with your project ID and save.

Challenge lab tasks completeed
  
  
